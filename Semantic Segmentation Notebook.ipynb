{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Semantic Segmentation Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import tensorflow as tf\n",
    "import helper\n",
    "import warnings\n",
    "from distutils.version import LooseVersion\n",
    "import project_tests as tests\n",
    "\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_vgg(sess, vgg_path):\n",
    "    \"\"\"\n",
    "    Load Pretrained VGG Model into TensorFlow.\n",
    "    :param sess: TensorFlow Session\n",
    "    :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n",
    "    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    #   Use tf.saved_model.loader.load to load the model and weights\n",
    "    vgg_tag = 'vgg16'\n",
    "    vgg_input_tensor_name = 'image_input:0'\n",
    "    vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
    "    vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
    "    vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
    "    vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
    "    #Get the saved model\n",
    "    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n",
    "    graph = tf.get_default_graph()\n",
    "    #Get input layer and keep prob parameter\n",
    "    layer0 = graph.get_tensor_by_name(vgg_input_tensor_name)\n",
    "    keep_prob = graph.get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
    "    #Get the rest of the layers\n",
    "    layer3 = graph.get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
    "    layer4 = graph.get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
    "    layer7 = graph.get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
    "\n",
    "    return layer0, keep_prob, layer3, layer4, layer7\n",
    "\n",
    "tests.test_load_vgg(load_vgg, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
    "    \"\"\"\n",
    "    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n",
    "    :param vgg_layer3_out: TF Tensor for VGG Layer 3 output\n",
    "    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n",
    "    :param vgg_layer7_out: TF Tensor for VGG Layer 7 output\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: The Tensor for the last layer of output\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    weights_initializer_stddev = 1e-2\n",
    "    weights_regularized_l2 = 1e-3\n",
    "    # 1x1 Convolutions to preserve spatial information that would\n",
    "    # be lost if kept the fully connected layers.\n",
    "    conv1x1_7 = tf.layers.conv2d(vgg_layer7_out,\n",
    "                                     num_classes,\n",
    "                                     1, # kernel_size\n",
    "                                     padding = 'same',\n",
    "                                     kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "                                     kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2),\n",
    "                                     name='conv1x1_7')\n",
    "    # Upsample deconvolution x 2\n",
    "    upsample2x_1 = tf.layers.conv2d_transpose(conv1x1_7,\n",
    "                                                  num_classes,\n",
    "                                                  4, # kernel_size\n",
    "                                                  strides= (2, 2),\n",
    "                                                  padding= 'same',\n",
    "                                                  kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "                                                  kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2),\n",
    "                                                  name='upsample2x_1')\n",
    "    conv1x1_4 = tf.layers.conv2d(vgg_layer4_out,\n",
    "                                     num_classes,\n",
    "                                     1, # kernel_size\n",
    "                                     padding = 'same',\n",
    "                                     kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "                                     kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2),\n",
    "                                     name='conv1x1_4')\n",
    "    # Adding skip layer.\n",
    "    skip1 = tf.add(upsample2x_1, conv1x1_4, name='skip1')\n",
    "    # Upsample deconvolutions x 2.\n",
    "    upsample2x_2 = tf.layers.conv2d_transpose(skip1,\n",
    "                                                   num_classes,\n",
    "                                                   4, # kernel_size\n",
    "                                                   strides= (2, 2),\n",
    "                                                   padding= 'same',\n",
    "                                                   kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "                                                   kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2),\n",
    "                                                   name='upsample2x_2')\n",
    "    conv1x1_3 = tf.layers.conv2d(vgg_layer3_out,\n",
    "                                     num_classes,\n",
    "                                     1, # kernel_size\n",
    "                                     padding = 'same',\n",
    "                                     kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "                                     kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2),\n",
    "                                     name='conv1x1_3')\n",
    "    # Adding skip layer.\n",
    "    skip2 = tf.add(upsample2x_2, conv1x1_3, name='skip2')\n",
    "    # Upsample deconvolution x 8.\n",
    "    upsample8x_3 = tf.layers.conv2d_transpose(skip2, num_classes, 16,\n",
    "                                                  strides= (8, 8),\n",
    "                                                  padding= 'same',\n",
    "                                                  kernel_initializer = tf.random_normal_initializer(stddev=weights_initializer_stddev),\n",
    "                                                  kernel_regularizer= tf.contrib.layers.l2_regularizer(weights_regularized_l2),\n",
    "                                                  name='upsample8x_3')\n",
    "    return upsample8x_3\n",
    "tests.test_layers(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n",
    "    \"\"\"\n",
    "    Build the TensorFLow loss and optimizer operations.\n",
    "    :param nn_last_layer: TF Tensor of the last layer in the neural network\n",
    "    :param correct_label: TF Placeholder for the correct label image\n",
    "    :param learning_rate: TF Placeholder for the learning rate\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: Tuple of (logits, train_op, cross_entropy_loss)\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    logits = tf.reshape(nn_last_layer, (-1, num_classes))\n",
    "    correct_label = tf.reshape(correct_label, (-1, num_classes))\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= logits, labels= correct_label))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "    train_op = optimizer.minimize(cross_entropy_loss)\n",
    "\n",
    "    return logits, train_op, cross_entropy_loss\n",
    "tests.test_optimize(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n",
    "             correct_label, keep_prob, learning_rate):\n",
    "    \"\"\"\n",
    "    Train neural network and print out the loss during training.\n",
    "    :param sess: TF Session\n",
    "    :param epochs: Number of epochs\n",
    "    :param batch_size: Batch size\n",
    "    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
    "    :param train_op: TF Operation to train the neural network\n",
    "    :param cross_entropy_loss: TF Tensor for the amount of loss\n",
    "    :param input_image: TF Placeholder for input images\n",
    "    :param correct_label: TF Placeholder for label images\n",
    "    :param keep_prob: TF Placeholder for dropout keep probability\n",
    "    :param learning_rate: TF Placeholder for learning rate\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print('Training!!!... with: {} epochs'.format(epochs))\n",
    "    print()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch : {}'.format(epoch))\n",
    "        loss_log = []\n",
    "        for image, label in get_batches_fn(batch_size):\n",
    "            aux, loss = sess.run([train_op, cross_entropy_loss],\n",
    "                                feed_dict={\n",
    "                                    input_image: image,\n",
    "                                    correct_label: label,\n",
    "                                    keep_prob: 0.5,\n",
    "                                    learning_rate: 1e-3\n",
    "                                })\n",
    "            loss_log.append('{:3f}'.format(loss))\n",
    "        print(\"Loss:\", loss_log)\n",
    "        #plt.plot(loss_log)\n",
    "        print()\n",
    "\n",
    "    print('Done!')\n",
    "    plt.plot(loss_log)\n",
    "    return None\n",
    "\n",
    "tests.test_train_nn(train_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    num_classes = 2\n",
    "    image_shape = (160, 576)\n",
    "    data_dir = './data'\n",
    "    runs_dir = './runs'\n",
    "    tests.test_for_kitti_dataset(data_dir)\n",
    "\n",
    "    # Download pretrained vgg model\n",
    "    helper.maybe_download_pretrained_vgg(data_dir)\n",
    "\n",
    "    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.\n",
    "    # You'll need a GPU with at least 10 teraFLOPS to train on.\n",
    "    #  https://www.cityscapes-dataset.com/\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Path to vgg model\n",
    "        vgg_path = os.path.join(data_dir, 'vgg')\n",
    "        # Create function to get batches\n",
    "        get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)\n",
    "\n",
    "        # OPTIONAL: Augment Images for better results\n",
    "        #  https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network\n",
    "\n",
    "        # TODO: Build NN using load_vgg, layers, and optimize function\n",
    "\n",
    "        correct_label = tf.placeholder(tf.int32, [None, None, None, num_classes], name='correct_label')\n",
    "        learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "        # Layers from vgg.\n",
    "        input_image, keep_prob, layer3_out, layer4_out, layer7_out = load_vgg(sess, vgg_path)\n",
    "\n",
    "        # New layers.\n",
    "        layer_output = layers(layer3_out, layer4_out, layer7_out, num_classes)\n",
    "\n",
    "\n",
    "        logits, train_op, cross_entropy_loss = optimize(layer_output, correct_label, learning_rate, num_classes)\n",
    "        # TODO: Train NN using the train_nn function\n",
    "        epochs = 42 \n",
    "        batch_size = 16\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n",
    "                 correct_label, keep_prob, learning_rate)\n",
    "\n",
    "        # TODO: Save inference data using helper.save_inference_samples\n",
    "        helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image)\n",
    "        #  helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image)\n",
    "\n",
    "        # OPTIONAL: Apply the trained model to a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\n",
      "TensorFlow Version: 1.0.1\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \n",
      "name: GeForce GTX 1080\n",
      "major: 6 minor: 1 memoryClockRate (GHz) 1.8095\n",
      "pciBusID 0000:01:00.0\n",
      "Total memory: 7.92GiB\n",
      "Free memory: 7.31GiB\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\n",
      "Default GPU Device: /gpu:0\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\n",
      "Tests Passed\n",
      "Tests Passed\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\n",
      "Tests Passed\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\n",
      "Tests Passed\n",
      "Tests Passed\n",
      "I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\n",
      "Training!!!... with: 40 epochs\n",
      "\n",
      "Epoch : 0\n",
      "W tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.16GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n",
      "Loss: ['1.431515', '6.107878', '0.783691', '0.864535', '0.770545', '0.697274', '0.687615', '0.706763', '0.695715', '0.665288', '0.652084', '0.644830', '0.647910', '0.627618', '0.591244', '0.582992', '0.552235', '0.527227', '0.491518']\n",
      "\n",
      "Epoch : 1\n",
      "Loss: ['0.485742', '0.464452', '0.428046', '0.453394', '0.416499', '0.342277', '0.394684', '0.338450', '0.374142', '0.373565', '0.349601', '0.313705', '0.284219', '0.290031', '0.278337', '0.256340', '0.343774', '0.325538', '0.212670']\n",
      "\n",
      "Epoch : 2\n",
      "Loss: ['0.399095', '0.291654', '0.456209', '0.312277', '0.262905', '0.304559', '0.290852', '0.283823', '0.274951', '0.237784', '0.293156', '0.282361', '0.237711', '0.234793', '0.215604', '0.200320', '0.220933', '0.205319', '0.128836']\n",
      "\n",
      "Epoch : 3\n",
      "Loss: ['0.210988', '0.243241', '0.193096', '0.206219', '0.192357', '0.203933', '0.168633', '0.221120', '0.194661', '0.173730', '0.180394', '0.174876', '0.196404', '0.182220', '0.180471', '0.193818', '0.218148', '0.154384', '0.158645']\n",
      "\n",
      "Epoch : 4\n",
      "Loss: ['0.178904', '0.205399', '0.152779', '0.209270', '0.185723', '0.138961', '0.203024', '0.139507', '0.155715', '0.162190', '0.166086', '0.136587', '0.141615', '0.110102', '0.186868', '0.146924', '0.172834', '0.129661', '0.208421']\n",
      "\n",
      "Epoch : 5\n",
      "Loss: ['0.182150', '0.163109', '0.149282', '0.166067', '0.164691', '0.139646', '0.163247', '0.172206', '0.151367', '0.139237', '0.150509', '0.168627', '0.122485', '0.134605', '0.159599', '0.157469', '0.132906', '0.128923', '0.087653']\n",
      "\n",
      "Epoch : 6\n",
      "Loss: ['0.206749', '0.166859', '0.118674', '0.129188', '0.153142', '0.141677', '0.139077', '0.143381', '0.138875', '0.157906', '0.131219', '0.144301', '0.137103', '0.130614', '0.151578', '0.133042', '0.124131', '0.119523', '0.094349']\n",
      "\n",
      "Epoch : 7\n",
      "Loss: ['0.234716', '0.165845', '0.139930', '0.139444', '0.158682', '0.164328', '0.157969', '0.148168', '0.119768', '0.144874', '0.148997', '0.117566', '0.150124', '0.149751', '0.149796', '0.120070', '0.129786', '0.125861', '0.142494']\n",
      "\n",
      "Epoch : 8\n",
      "Loss: ['0.123876', '0.139572', '0.141610', '0.121232', '0.137967', '0.112629', '0.109667', '0.145110', '0.111421', '0.116548', '0.150260', '0.136031', '0.154763', '0.108914', '0.115501', '0.148332', '0.141784', '0.111530', '0.179099']\n",
      "\n",
      "Epoch : 9\n",
      "Loss: ['0.135959', '0.142840', '0.128630', '0.093976', '0.118764', '0.133340', '0.130084', '0.111684', '0.116264', '0.119398', '0.112387', '0.089781', '0.117503', '0.119526', '0.111560', '0.115825', '0.113606', '0.103976', '0.152262']\n",
      "\n",
      "Epoch : 10\n",
      "Loss: ['0.105600', '0.088424', '0.129896', '0.128487', '0.118570', '0.122125', '0.142494', '0.121636', '0.133445', '0.127906', '0.096864', '0.116254', '0.119193', '0.113580', '0.115177', '0.106901', '0.087525', '0.134631', '0.101157']\n",
      "\n",
      "Epoch : 11\n",
      "Loss: ['0.103844', '0.087134', '0.107471', '0.113477', '0.097041', '0.099717', '0.134707', '0.093086', '0.115082', '0.103128', '0.101778', '0.111636', '0.088413', '0.104933', '0.087706', '0.091587', '0.090348', '0.085620', '0.106382']\n",
      "\n",
      "Epoch : 12\n",
      "Loss: ['0.131368', '0.120919', '0.110765', '0.085712', '0.096034', '0.077547', '0.085823', '0.087838', '0.105277', '0.098977', '0.095731', '0.077816', '0.106463', '0.101692', '0.102658', '0.119931', '0.116701', '0.100594', '0.084055']\n",
      "\n",
      "Epoch : 13\n",
      "Loss: ['0.106199', '0.115215', '0.126012', '0.087259', '0.094630', '0.126266', '0.100609', '0.102998', '0.110860', '0.098652', '0.105531', '0.113982', '0.127733', '0.109335', '0.095068', '0.078459', '0.096035', '0.090684', '0.143527']\n",
      "\n",
      "Epoch : 14\n",
      "Loss: ['0.113785', '0.115744', '0.088806', '0.090027', '0.119958', '0.091103', '0.109507', '0.127220', '0.104587', '0.091382', '0.094095', '0.112995', '0.082923', '0.086320', '0.071271', '0.092809', '0.102694', '0.086242', '0.036735']\n",
      "\n",
      "Epoch : 15\n",
      "Loss: ['0.090436', '0.109214', '0.096764', '0.094133', '0.127673', '0.076024', '0.122235', '0.105795', '0.104985', '0.085503', '0.091650', '0.102671', '0.094530', '0.060516', '0.080964', '0.073789', '0.085190', '0.092447', '0.073048']\n",
      "\n",
      "Epoch : 16\n",
      "Loss: ['0.081619', '0.110358', '0.068581', '0.082367', '0.080225', '0.068225', '0.077932', '0.089501', '0.088149', '0.068728', '0.074365', '0.088561', '0.111767', '0.071619', '0.102070', '0.108533', '0.096812', '0.081560', '0.053872']\n",
      "\n",
      "Epoch : 17\n",
      "Loss: ['0.085857', '0.086817', '0.092467', '0.086953', '0.080373', '0.082149', '0.075962', '0.089286', '0.077011', '0.101853', '0.071655', '0.092176', '0.071208', '0.077653', '0.078005', '0.076099', '0.075586', '0.071746', '0.067666']\n",
      "\n",
      "Epoch : 18\n",
      "Loss: ['0.075703', '0.098221', '0.069143', '0.073189', '0.090541', '0.094228', '0.060812', '0.077311', '0.066323', '0.082714', '0.072656', '0.057910', '0.080448', '0.068742', '0.096721', '0.071522', '0.062181', '0.084058', '0.026799']\n",
      "\n",
      "Epoch : 19\n",
      "Loss: ['0.077329', '0.067217', '0.063051', '0.058232', '0.079364', '0.058338', '0.065631', '0.085441', '0.063774', '0.065523', '0.068296', '0.072039', '0.056399', '0.071053', '0.084989', '0.080131', '0.069600', '0.074734', '0.043594']\n",
      "\n",
      "Epoch : 20\n",
      "Loss: ['0.081796', '0.074665', '0.062148', '0.063865', '0.062275', '0.065864', '0.068566', '0.075626', '0.071612', '0.060779', '0.066041', '0.066422', '0.062986', '0.062659', '0.074471', '0.068795', '0.076252', '0.057175', '0.077641']\n",
      "\n",
      "Epoch : 21\n",
      "Loss: ['0.062355', '0.095958', '0.065029', '0.061135', '0.079283', '0.064474', '0.059655', '0.064812', '0.050445', '0.064389', '0.063286', '0.062915', '0.070522', '0.054371', '0.068373', '0.069840', '0.065477', '0.066192', '0.059656']\n",
      "\n",
      "Epoch : 22\n",
      "Loss: ['0.140042', '0.061469', '0.113887', '0.075960', '0.082044', '0.081614', '0.087449', '0.099368', '0.070319', '0.092299', '0.096205', '0.083843', '0.096114', '0.088700', '0.075999', '0.077622', '0.090681', '0.079216', '0.047699']\n",
      "\n",
      "Epoch : 23\n",
      "Loss: ['0.113353', '0.076131', '0.065767', '0.123016', '0.095928', '0.074764', '0.072517', '0.069931', '0.082460', '0.055905', '0.053140', '0.093432', '0.074033', '0.077976', '0.083025', '0.065696', '0.067636', '0.076060', '0.062158']\n",
      "\n",
      "Epoch : 24\n",
      "Loss: ['0.064650', '0.088674', '0.059044', '0.068973', '0.071938', '0.059373', '0.059164', '0.056608', '0.073161', '0.058931', '0.061989', '0.047248', '0.059613', '0.080733', '0.068395', '0.080216', '0.089785', '0.072163', '0.192953']\n",
      "\n",
      "Epoch : 25\n",
      "Loss: ['0.066445', '0.064396', '0.071402', '0.065787', '0.083057', '0.065118', '0.072857', '0.075688', '0.056076', '0.070341', '0.075108', '0.085859', '0.059818', '0.066721', '0.080316', '0.058730', '0.064654', '0.064714', '0.038214']\n",
      "\n",
      "Epoch : 26\n",
      "Loss: ['0.086712', '0.072444', '0.063523', '0.072760', '0.092550', '0.065369', '0.071844', '0.056329', '0.077850', '0.075488', '0.060214', '0.056812', '0.058595', '0.069097', '0.054898', '0.055883', '0.092375', '0.057634', '0.024069']\n",
      "\n",
      "Epoch : 27\n",
      "Loss: ['0.065200', '0.069339', '0.059025', '0.062407', '0.059643', '0.056595', '0.067873', '0.047766', '0.056695', '0.047537', '0.051643', '0.071420', '0.050963', '0.043720', '0.049681', '0.054345', '0.053982', '0.059858', '0.066415']\n",
      "\n",
      "Epoch : 28\n",
      "Loss: ['0.051463', '0.059035', '0.051263', '0.051918', '0.038727', '0.070549', '0.057505', '0.055274', '0.075176', '0.049966', '0.059257', '0.059795', '0.081186', '0.059990', '0.048076', '0.069806', '0.060287', '0.049204', '0.054245']\n",
      "\n",
      "Epoch : 29\n",
      "Loss: ['0.059092', '0.059449', '0.052468', '0.048818', '0.040698', '0.056685', '0.048245', '0.042684', '0.061305', '0.063692', '0.061190', '0.057970', '0.059518', '0.064961', '0.052046', '0.049581', '0.048654', '0.048552', '0.044158']\n",
      "\n",
      "Epoch : 30\n",
      "Loss: ['0.055746', '0.047190', '0.038150', '0.057509', '0.052445', '0.051852', '0.056484', '0.041586', '0.048368', '0.065132', '0.053349', '0.048213', '0.053337', '0.047779', '0.043234', '0.059478', '0.053258', '0.066094', '0.056410']\n",
      "\n",
      "Epoch : 31\n",
      "Loss: ['0.051414', '0.060272', '0.066576', '0.049067', '0.054107', '0.065421', '0.055098', '0.064661', '0.062913', '0.052681', '0.062361', '0.049572', '0.059273', '0.038848', '0.052607', '0.049660', '0.068535', '0.068269', '0.030936']\n",
      "\n",
      "Epoch : 32\n",
      "Loss: ['0.055277', '0.058029', '0.050203', '0.049032', '0.057645', '0.052521', '0.050564', '0.053083', '0.056276', '0.052526', '0.052034', '0.035768', '0.044549', '0.049102', '0.053622', '0.046068', '0.045199', '0.057737', '0.048610']\n",
      "\n",
      "Epoch : 33\n",
      "Loss: ['0.046127', '0.055987', '0.037217', '0.056977', '0.052359', '0.045916', '0.040173', '0.047947', '0.047965', '0.037948', '0.049615', '0.076305', '0.063217', '0.043741', '0.047574', '0.061129', '0.050484', '0.045207', '0.047415']\n",
      "\n",
      "Epoch : 34\n",
      "Loss: ['0.065369', '0.041986', '0.064086', '0.050861', '0.053387', '0.046592', '0.052050', '0.053479', '0.054600', '0.042481', '0.044969', '0.045983', '0.042128', '0.064969', '0.032135', '0.050306', '0.047715', '0.045483', '0.104303']\n",
      "\n",
      "Epoch : 35\n",
      "Loss: ['0.050046', '0.052376', '0.060068', '0.060828', '0.046546', '0.050316', '0.059011', '0.042748', '0.044705', '0.061071', '0.057511', '0.057227', '0.045730', '0.054738', '0.050539', '0.038853', '0.043943', '0.039330', '0.074562']\n",
      "\n",
      "Epoch : 36\n",
      "Loss: ['0.054717', '0.050305', '0.045393', '0.061757', '0.040249', '0.042198', '0.053183', '0.055544', '0.047988', '0.037017', '0.041199', '0.051166', '0.049580', '0.041969', '0.054263', '0.053704', '0.036486', '0.053028', '0.041184']\n",
      "\n",
      "Epoch : 37\n",
      "Loss: ['0.050980', '0.038970', '0.051511', '0.042841', '0.045713', '0.059763', '0.038649', '0.046407', '0.047038', '0.048022', '0.051847', '0.039201', '0.041878', '0.042419', '0.047415', '0.047582', '0.034052', '0.052244', '0.075687']\n",
      "\n",
      "Epoch : 38\n",
      "Loss: ['0.052214', '0.033855', '0.049936', '0.041389', '0.050871', '0.049835', '0.037930', '0.040322', '0.057322', '0.045740', '0.041786', '0.049549', '0.043751', '0.046352', '0.043456', '0.042436', '0.040093', '0.041316', '0.018976']\n",
      "\n",
      "Epoch : 39\n",
      "Loss: ['0.034183', '0.042327', '0.037950', '0.047087', '0.039832', '0.035079', '0.042399', '0.038873', '0.044959', '0.038199', '0.043016', '0.057094', '0.039163', '0.048549', '0.038422', '0.036957', '0.040519', '0.050871', '0.038139']\n",
      "\n",
      "Done!\n",
      "Training Finished. Saving test images to: ./runs/1529351360.4743564\n"
     ]
    }
   ],
   "source": [
    "!python3 main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
